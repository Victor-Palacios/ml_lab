{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attached-action",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overfitting-&amp;-Generalization\" data-toc-modified-id=\"Overfitting-&amp;-Generalization-1\">Overfitting &amp; Generalization</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#-Common-Data-Training-Pattern\" data-toc-modified-id=\"-Common-Data-Training-Pattern-3\"> Common Data Training Pattern</a></span></li><li><span><a href=\"#What-should-you-do?\" data-toc-modified-id=\"What-should-you-do?-4\">What should you do?</a></span></li><li><span><a href=\"#-Overall---Keep-the-goal-the-goal\" data-toc-modified-id=\"-Overall---Keep-the-goal-the-goal-5\"> Overall - Keep the goal the goal</a></span></li><li><span><a href=\"#Combinations-of-training-vs-test-error\" data-toc-modified-id=\"Combinations-of-training-vs-test-error-6\">Combinations of training vs test error</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-7\">Check for understanding</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-8\">Student Activity</a></span></li><li><span><a href=\"#Factors-Beyond-Evaluation-Metrics\" data-toc-modified-id=\"Factors-Beyond-Evaluation-Metrics-9\">Factors Beyond Evaluation Metrics</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-10\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-11\">Bonus Material</a></span></li><li><span><a href=\"#Learning-Curves-in-scikit-learn\" data-toc-modified-id=\"Learning-Curves-in-scikit-learn-12\">Learning Curves in scikit-learn</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-harmony",
   "metadata": {},
   "source": [
    "<center><h2>Overfitting & Generalization</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"../images/overfitting_metaphor.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-evidence",
   "metadata": {},
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define overfitting and generalization, both in your words and technically.\n",
    "- Make quantitative decisions to choose between machine learning models.\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-commons",
   "metadata": {},
   "source": [
    "<center><h2> Common Data Training Pattern</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/cross val error.png\" width=\"75%\"/></center>\n",
    "\n",
    "1. Initially errors on each of the 3-way splits goes down (Train, Validation, and Test)\n",
    "2. At some point, Training error keeps going down. However, Validation and Test error remains constant.\n",
    "3. Then, Validation and Test error goes up (diverges from Training error).\n",
    "\n",
    "Overfitting is increased errors in Validation or Test data compared to error Training data that is beyond chance.  \n",
    "The error bars on the Validation data are an estimate of chance.\n",
    "\n",
    "Generalization is a model’s ability to predict unseen data.\n",
    "\n",
    "[Source](https://wp.wwu.edu/machinelearning/2017/01/22/generalization-and-overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-soviet",
   "metadata": {},
   "source": [
    "<center><h2>What should you do?</h2></center>\n",
    "\n",
    "- Definitely stop training when validation error starts to go up.\n",
    "- You can stop training if validation error is not going down. This will result in a more parsimonious model with possibly more error.\n",
    "\n",
    "This pattern extends across different types of training:\n",
    "\n",
    "- Number of training iterations (e.g., stochastic gradient descent)\n",
    "- Model complexity (e.g., adding polynominal features)\n",
    "- And more…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-architecture",
   "metadata": {},
   "source": [
    "<center><h2> Overall - Keep the goal the goal</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"../images/graph_overfitting_underfitting.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-bachelor",
   "metadata": {},
   "source": [
    " The goal of machine learning is learn a function from data that can generalize to unseen data.\n",
    " \n",
    " We want minimal test error. We take training error and validation error as proxies for test error. \n",
    " \n",
    "If we are an oracle - We should take model between 4 and 8 because that has minimal test error. However, we can tune a model on test data. Thus we use validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-bullet",
   "metadata": {},
   "source": [
    "<center><h2>Combinations of training vs test error</h2></center>\n",
    "\n",
    "<center><img src=\"../images/training vs test error.png\" width=\"75%\"/></center>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-mississippi",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "If you score to 100% on the training dataset, is the model overfit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-elizabeth",
   "metadata": {},
   "source": [
    "Nope! Overfitting is not judged by absolute performance on the training dataset.\n",
    "\n",
    "If that model also get 100% on validation, it just a super awesome model. \"Learn a function from data that can generalize to unseen data.\" Sometimes that function has perfect performance. Easy day in machine learning!\n",
    "\n",
    "> At the end of the day, training a machine learning model is like studying for a test. You (the model) use learning resources such as books, past exams, flash cards etc. (train set) to perform well on a test/exam (test set). Knowing your learning resources perfectly doesn't mean you are overfitting. You would be overfitting if this is all you knew and couldn't perform well on the exam at all.\n",
    "\n",
    "Source - https://datascience.stackexchange.com/questions/74682/is-over-fitting-okay-if-test-accuracy-is-high-enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-death",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "In small groups - Decide which model you would choose? Why?\n",
    "\n",
    "We are measuring accuracy so higher numbers are better.\n",
    "\n",
    "| Model   | Training |  Validation |\n",
    "|:-------:|:------:| :---:|\n",
    "| A       | 19% | 20% |\n",
    "| B       | 21% | 20% |\n",
    "\n",
    "\n",
    "| Model   | Training |  Validation 1 |  Validation 2 |  Validation 3 |\n",
    "|:-------:|:--------:| :------------:| :------------:| :------------:| \n",
    "| C       | 80% | 73% | 72% | 74%  |\n",
    "| D       | 80% | 74% | 65% | 79%  |\n",
    "\n",
    "| Model   | Training |  Validation |\n",
    "|:-------:|:------:| :---:|\n",
    "| E       | 94% | 80% |\n",
    "| F       | 99% | 85% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "given-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "1.0\n",
      "72.66666666666667\n",
      "7.094598884597588\n"
     ]
    }
   ],
   "source": [
    "# Solutions \n",
    "\n",
    "# A vs B\n",
    "# I would flip a coin between model A and B. They both are equally bad. \n",
    "# I would think about improve model fitting in general, aka lower bias.\n",
    "\n",
    "# C vs D\n",
    "# Run some descriptive stats\n",
    "from statistics import mean, stdev\n",
    "\n",
    "model_c = [73, 72, 74]\n",
    "print(mean(model_c))\n",
    "print(stdev(model_c))\n",
    "\n",
    "model_d = [74, 65, 79]\n",
    "print(mean(model_d))\n",
    "print(stdev(model_d))\n",
    "\n",
    "# I would choose model C. It is slightly worse (could be do to chance).\n",
    "# However, I would not trust a high variance model. I would not put it into production.\n",
    "\n",
    "\n",
    "# E vs F\n",
    "# I would pick model F. \n",
    "# Model F has the highest validation performance. \n",
    "# Even though, model F is more ovefit than model E. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-letter",
   "metadata": {},
   "source": [
    "> It is common in applied machine learning to have the model with the lowest generalization error, as measured by score on validation data, also have the biggest delta from the score on the training data.\n",
    "\n",
    "> There is nothing inherently wrong with overfitting, it depends on the goal of the project. The typical goal of applied machine learning is high predictive ability on unseen data, aka low generalization error. It is okay if the model \"memorizes\" more of the training data if that helps the model improve generalization.\"\n",
    "\n",
    "\n",
    "Overfitting is not good or bad, it is just a thing that happens. The empirical estimation of overfitting is doing worse on unseen data than on training data. The primary goal of machine learning is predicting unseen data.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Reconciling modern machine-learning practice and the classical bias–variance trade-off](https://www.pnas.org/content/116/32/15849)\n",
    "- https://datascience.stackexchange.com/questions/66350/what-would-i-prefer-an-over-fitted-model-or-a-less-accurate-model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-decimal",
   "metadata": {},
   "source": [
    "<center><h2>Factors Beyond Evaluation Metrics</h2></center>\n",
    " \n",
    "Given two models that have the same out-of-sample performance but different in-sample performance, you should would go with the simpler model. \n",
    "\n",
    "In other words, you may get non-evaluation metric performance gain by going with the simpler model:\n",
    "\n",
    "- The simpler model might have few hidden flaws.\n",
    "- The simpler model may have decreased computing time.\n",
    "- There still might overfitting you have not measure yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-router",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-bachelor",
   "metadata": {},
   "source": [
    " <center><h2>Takeaways</h2></center>\n",
    " \n",
    " - Machine learning is about predictive ability on unseen data (generalization). Choose the model with lowest validation error.\n",
    " - You want both low bias and low variance. First, aim for low bias. Second, aim for low variance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-cigarette",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-incidence",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-chrome",
   "metadata": {},
   "source": [
    "<center><h2>Learning Curves in scikit-learn</h2></center>\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-arrangement",
   "metadata": {},
   "source": [
    "Here is model this purposely overfit\n",
    "\n",
    "https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-bubble",
   "metadata": {},
   "source": [
    "__WARNING - BRIAN'S OPINION (NOT FACTS)__\n",
    "\n",
    "Personally, I do not care about overfitting. I mostly care about generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-printer",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
