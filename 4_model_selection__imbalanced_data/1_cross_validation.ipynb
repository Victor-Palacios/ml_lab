{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-1\">Cross Validation</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#What-is-the-goal-of-Machine-Learning?\" data-toc-modified-id=\"What-is-the-goal-of-Machine-Learning?-3\">What is the goal of Machine Learning?</a></span></li><li><span><a href=\"#3-Way-Data-Split\" data-toc-modified-id=\"3-Way-Data-Split-4\">3-Way Data Split</a></span></li><li><span><a href=\"#Different-Model-Evaluation-Procedures-\" data-toc-modified-id=\"Different-Model-Evaluation-Procedures--5\">Different Model Evaluation Procedures </a></span></li><li><span><a href=\"#Training-and-testing-on-the-same-data\" data-toc-modified-id=\"Training-and-testing-on-the-same-data-6\">Training and testing on the same data</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-7\">Check for understanding</a></span></li><li><span><a href=\"#Train/test-split\" data-toc-modified-id=\"Train/test-split-8\">Train/test split</a></span></li><li><span><a href=\"#What-percentage-of-the-data-should-you-hold-out-for-testing?\" data-toc-modified-id=\"What-percentage-of-the-data-should-you-hold-out-for-testing?-9\">What percentage of the data should you hold-out for testing?</a></span></li><li><span><a href=\"#Big-Data-Split\" data-toc-modified-id=\"Big-Data-Split-10\">Big Data Split</a></span></li><li><span><a href=\"#Common-train-/-test-splits\" data-toc-modified-id=\"Common-train-/-test-splits-11\">Common train / test splits</a></span></li><li><span><a href=\"#3-way-Split:-Train/Test/Validation\" data-toc-modified-id=\"3-way-Split:-Train/Test/Validation-12\">3-way Split: Train/Test/Validation</a></span></li><li><span><a href=\"#Most-Common-Way-to-Use-Data\" data-toc-modified-id=\"Most-Common-Way-to-Use-Data-13\">Most Common Way to Use Data</a></span></li><li><span><a href=\"#Common-uses-of-validation-set\" data-toc-modified-id=\"Common-uses-of-validation-set-14\">Common uses of validation set</a></span></li><li><span><a href=\"#k-fold-CV\" data-toc-modified-id=\"k-fold-CV-15\">k-fold CV</a></span></li><li><span><a href=\"#What-should-k-be-for-k-fold-cross-validation-(CV)?\" data-toc-modified-id=\"What-should-k-be-for-k-fold-cross-validation-(CV)?-16\">What should k be for k-fold cross validation (CV)?</a></span></li><li><span><a href=\"#Protip---Stratified-Sampling\" data-toc-modified-id=\"Protip---Stratified-Sampling-17\">Protip - Stratified Sampling</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-18\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-19\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-20\">Bonus Material</a></span></li><li><span><a href=\"#Leave-one-out-Cross-Validation-(LOOCV)\" data-toc-modified-id=\"Leave-one-out-Cross-Validation-(LOOCV)-21\">Leave-one-out Cross-Validation (LOOCV)</a></span></li><li><span><a href=\"#When-should-I-use-LOOCV?\" data-toc-modified-id=\"When-should-I-use-LOOCV?-22\">When should I use LOOCV?</a></span></li><li><span><a href=\"#LabelKFold\" data-toc-modified-id=\"LabelKFold-23\">LabelKFold</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Cross Validation</h2></center>\n",
    "<center><img src=\"https://i.stack.imgur.com/c6ECF.png\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain 3-way split between training, validation, and test datasets.\n",
    "- Explain the purpose of each type of dataset.\n",
    "- Describe k-fold cross validation (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the goal of Machine Learning?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Learn a function from data that can generalize to novel data.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>3-Way Data Split</h2></center>\n",
    "\n",
    "<center><img src=\"../images/three_way_data_split.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Training Set is to fit the parameters (i.e., model weights).\n",
    "- Validation set is to tune the hyper-parameters.\n",
    "- Test Set is to assess the performance of the model (i.e., evaluating the predictive power / generalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Different Model Evaluation Procedures </h2></center>\n",
    "\n",
    "1. Training and testing on the same data\n",
    "1. Train/test split\n",
    "1. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Training and testing on the same data</h2></center>\n",
    "\n",
    "Train on __all__ your data. Consider your performance on training data the best evaluation of model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is only having training data a bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluating only on training data will encourage you to overfit (low bias, high variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "When would you want to train and test on the same data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. \"Small data\" - Too few rows to split up. \n",
    "2. The domain is static - new data will be exactly the same as the training data.\n",
    "3. Too little time (e.g., learning in a low-latency system).\n",
    "\n",
    "One practical example are headphones that learn to noise block. They are low computation, no memory, and low latency. There is no test dataset. Just train on the streaming data as it comes in. https://bragi.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Train/test split</h2></center>\n",
    "\n",
    "<center><img src=\"../images/test.png\" width=\"70%\"/></center>\n",
    "\n",
    "Split the dataset into two sets\n",
    "\n",
    "1. Training set: Data points used to train the model.\n",
    "1. Testing set: Data points used to check the performance once training is __completely finished__.\n",
    "\n",
    "In other words, the model is trained and tested on different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What percentage of the data should you hold-out for testing?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mostly an empirical choice based on domain complexity and size of the data. Number of observations (n) and number of features (p) both matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hopefully, you have \"Medium Data\".\n",
    "\n",
    "For example, if you only have 100 examples, a .90 train / .10 test split does not make a lot of sense.\n",
    "\n",
    "If you have 1M examples, a .90 train / .10 test split might make sense because there are 100,000 examples in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Big Data Split</h2></center>\n",
    "\n",
    "> But in the era of big data where we now have machine learning problems with sometimes more than a billion examples, the fraction of data allocated to test sets has been shrinking, even as the absolute number of examples in the test sets has been growing. \n",
    "\n",
    "> There is no need to have excessively large test sets beyond what is needed to evaluate the performance of your algorithms.\n",
    "\n",
    "Source: Andrew Ng's Machine Learning Yearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Common train / test splits</h2></center>\n",
    "\n",
    "- 70% train / 30% test\n",
    "- 80% train / 20% test\n",
    "- 90% train / 10% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>3-way Split: Train/Test/Validation</h2></center>\n",
    "\n",
    "<center><img src=\"../images/data_complete.png\" width=\"75%\"/></center>\n",
    "<center><img src=\"../images/split.png\" width=\"75%\"/></center>\n",
    "\n",
    "Split your data into a 3 separate sets:\n",
    "\n",
    "1. Test set - Final dataset for one-time evaluation.\n",
    "1. Training set - Dataset for repeated training.\n",
    "1. Validation set - Paired with the training dataset to evaluate performance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Validiation dataset is also called the Development dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Most Common Way to Use Data</h2></center>\n",
    "\n",
    "1. Train / test split at beginning.\n",
    "2. Find best model with cross validation on k-folds of the training data.\n",
    "3. Train the final model with all of the training data.\n",
    "4. See on the final model performs by looking at the test data once.\n",
    "\n",
    "<center><img src=\"../images/cross_validation.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Common uses of validation set</h2></center>\n",
    "\n",
    "<center><img src=\"../images/validation_dataset.png\" width=\"70%\"/></center>\n",
    "\n",
    "1) Compare different hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Compare different features.   \n",
    "\n",
    "3) Compare different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) Estimate Variance (and calculate error bars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://www.quora.com/What-is-the-definition-of-development-set-in-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load and define data\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406\n"
     ]
    }
   ],
   "source": [
    "# Performance on training dataset only\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "print(f\"{lm.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Performance on test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "print(f\"{lm.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': 1000,\n",
       " 'normalize': False,\n",
       " 'positive': False,\n",
       " 'precompute': False,\n",
       " 'random_state': None,\n",
       " 'selection': 'cyclic',\n",
       " 'tol': 0.0001,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lm = Lasso()\n",
    "lm.fit(X_train, y_train)\n",
    "lm.get_params() # Note alpha is the default value of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6864832088331325"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso with automated search via cross validation\n",
    "from sklearn.linear_model import LassoCV\n",
    "# Lasso linear model with iterative fitting along a regularization path.\n",
    "\n",
    "lm = LassoCV(cv=5)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# The amount of penalization chosen by cross validation\n",
    "# This amount of regularization has best performance on hold-out data set.\n",
    "lm.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67609439, 0.49437898, 0.66236367, 0.58923337, 0.68794944,\n",
       "       0.69982627, 0.73273511, 0.5670373 , 0.76375619, 0.61140566])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation can estimate the variance of your model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(estimator=Lasso(), \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>k-fold CV</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/validation_dataset.png\" width=\"100%\"/></center>\n",
    "\n",
    "The training set is split into k number of smaller sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration            Training set observations             Validate set observations\n",
      "    1     [ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16 18] [ 5  8 17 19]\n",
      "    2     [ 1  3  4  5  6  7  8  9 10 11 12 15 16 17 18 19] [ 0  2 13 14]\n",
      "    3     [ 0  1  2  3  4  5  6  7  8 12 13 14 16 17 18 19] [ 9 10 11 15]\n",
      "    4     [ 0  1  2  4  5  6  8  9 10 11 13 14 15 16 17 19] [ 3  7 12 18]\n",
      "    5     [ 0  2  3  5  7  8  9 10 11 12 13 14 15 17 18 19] [ 1  4  6 16]\n"
     ]
    }
   ],
   "source": [
    "# Simulate splitting a dataset of observations into 5 folds\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True).split(range(20))\n",
    "\n",
    "print(f\"{'Iteration'} {'Training set observations':^48} {'Validate set observations'}\")\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print(f\"{iteration:^9} {data[0]} {data[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What should k be for k-fold cross validation (CV)?</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again, an empirical choice based on how many variations of an algorithm you want to explore.\n",
    "\n",
    "`k=10` tends to be the most popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Protip - Stratified Sampling</h2></center>\n",
    "\n",
    "<center><img src=\"../images/XJZve.png\" width=\"70%\"/></center>\n",
    "<br>\n",
    "\n",
    "For classification problems, stratified sampling is recommended for creating the folds.\n",
    "\n",
    "Imbalanced classes will be proportionally represented in each CV fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "scikit-learn's `cross_val_score` function does this by default\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/45969390/difference-between-stratifiedkfold-and-stratifiedshufflesplit-in-sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Always do train/test splits.\n",
    "- Do k-fold cross validation (CV) whenever possible. \n",
    "- `scikit-learn` makes it easy to do the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Leave-one-out Cross-Validation (LOOCV)</h2></center>\n",
    "\n",
    "<center><img src=\"../images/loocv.png\" width=\"70%\"/></center>\n",
    "\n",
    "<center>A special case of k-fold CV is when k=n.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- LOOCV is computationally __less__ intensive because the model only predicts on single instance.\n",
    "\n",
    "- LOOCV is computationally __more__ intensive because the model could be tried n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Performing LOOCV multiple times always yields the same results because there is no randomness in the training/validation set splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>When should I use LOOCV?</h2></center>\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Simple - just a pick a single data example\n",
    "- Fast - predict on a only single data example\n",
    "- Works with small data - Useful if you have a tiny dataset where you can not afford a large validation set.\n",
    "\n",
    "\n",
    "Cons:\n",
    "    \n",
    "- No good estimate. What conclusions would you draw from a single data point?\n",
    "\n",
    "You can pick your p. In most machine learning use cases, p should not be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: An Introduction to Statistical Learning by by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>LabelKFold</h2></center>\n",
    "\n",
    "> The same label will not appear in two different folds (the number of distinct labels has to be at least equal to the number of folds).\n",
    "\n",
    ">The folds are approximately balanced in the sense that the number of distinct labels is approximately the same in each fold.\n",
    "\n",
    "Sources:\n",
    "- http://vighneshbirodkar.github.io/scikit-learn.github.io/dev/modules/generated/sklearn.model_selection.LabelKFold.html\n",
    "- https://youtu.be/7l_WQO3JbWE?t=534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "---- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
