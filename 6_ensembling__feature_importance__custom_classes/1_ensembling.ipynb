{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Ensemble-Learning---The-Wisdom-of-the-Crowds\" data-toc-modified-id=\"Ensemble-Learning---The-Wisdom-of-the-Crowds-1\">Ensemble Learning - The Wisdom of the Crowds</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Ensemble-Methods,-aka-Ensembling\" data-toc-modified-id=\"Ensemble-Methods,-aka-Ensembling-3\">Ensemble Methods, aka Ensembling</a></span></li><li><span><a href=\"#How-the-weak--become-strong-together\" data-toc-modified-id=\"How-the-weak--become-strong-together-4\">How the weak  become strong together</a></span></li><li><span><a href=\"#Student-Activity:-Think,-Pair,-&amp;-Share\" data-toc-modified-id=\"Student-Activity:-Think,-Pair,-&amp;-Share-5\">Student Activity: Think, Pair, &amp; Share</a></span></li><li><span><a href=\"#Voting-can-work-with-heterogeneous-algorithms-\" data-toc-modified-id=\"Voting-can-work-with-heterogeneous-algorithms--6\">Voting can work with heterogeneous algorithms </a></span></li><li><span><a href=\"#Types-of-Voting\" data-toc-modified-id=\"Types-of-Voting-7\">Types of Voting</a></span></li><li><span><a href=\"#Bagging,-aka-Bootstrap-Aggregating\" data-toc-modified-id=\"Bagging,-aka-Bootstrap-Aggregating-8\">Bagging, aka Bootstrap Aggregating</a></span></li><li><span><a href=\"#Bootstrap-Statistical-Procedure\" data-toc-modified-id=\"Bootstrap-Statistical-Procedure-9\">Bootstrap Statistical Procedure</a></span></li><li><span><a href=\"#Bootstrap-Sampling-Steps\" data-toc-modified-id=\"Bootstrap-Sampling-Steps-10\">Bootstrap Sampling Steps</a></span></li><li><span><a href=\"#Bootstraping\" data-toc-modified-id=\"Bootstraping-11\">Bootstraping</a></span></li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-12\">Bagging</a></span></li><li><span><a href=\"#Why-do-Bagging?\" data-toc-modified-id=\"Why-do-Bagging?-13\">Why do Bagging?</a></span></li><li><span><a href=\"#Stacking-(not-just-for-Transformers)-\" data-toc-modified-id=\"Stacking-(not-just-for-Transformers)--14\">Stacking (not just for Transformers) </a></span></li><li><span><a href=\"#Common-Examples-of-Pipeline-Stacking\" data-toc-modified-id=\"Common-Examples-of-Pipeline-Stacking-15\">Common Examples of Pipeline Stacking</a></span></li><li><span><a href=\"#Stacking-with-a-Metalearner\" data-toc-modified-id=\"Stacking-with-a-Metalearner-16\">Stacking with a Metalearner</a></span></li><li><span><a href=\"#Challenge-Question\" data-toc-modified-id=\"Challenge-Question-17\">Challenge Question</a></span></li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-18\">Boosting</a></span></li><li><span><a href=\"#Boosting-algorithms\" data-toc-modified-id=\"Boosting-algorithms-19\">Boosting algorithms</a></span></li><li><span><a href=\"#-ML-2-will-cover-boosting\" data-toc-modified-id=\"-ML-2-will-cover-boosting-20\"> ML 2 will cover boosting</a></span></li><li><span><a href=\"#When-should-you-choose-Stacking?\" data-toc-modified-id=\"When-should-you-choose-Stacking?-21\">When should you choose Stacking?</a></span></li><li><span><a href=\"#When-should-you-choose-Bagging?\" data-toc-modified-id=\"When-should-you-choose-Bagging?-22\">When should you choose Bagging?</a></span></li><li><span><a href=\"#When-should-you-choose-Boosting?\" data-toc-modified-id=\"When-should-you-choose-Boosting?-23\">When should you choose Boosting?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-24\">Check for understanding</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-25\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-26\">Bonus Material</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-27\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-28\">Check for understanding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Ensemble Learning - The Wisdom of the Crowds</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/crowds.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define ensembling in your own words.\n",
    "- Explain why ensembling is a useful ML technique.\n",
    "- Define and give examples of 3 most common ensembling methods:\n",
    "    1. Stacking\n",
    "    2. Bagging\n",
    "    3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ensemble Methods, aka Ensembling\n",
    "------\n",
    "\n",
    "Combine multiple ML models to obtain better predictive performance than any of single models could do alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Techniques for combining several weak learners to produce a single strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How the weak  become strong together\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"http://jasonleaster.github.io/images/img_for_2015_12_13/stump.png\" width=\"55%\"/></center>\n",
    "\n",
    "Suppose we have a decision stump classifier with 70% accuracy, but Bayes Error is 0%.   \n",
    "Is this good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why I have group assignments. Each person might individually a \"weak\" learner (hopefully independent). Taking a simple majority vote, together the group-level predictions are more likely to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Student Activity: Think, Pair, & Share\n",
    "------\n",
    "\n",
    "Suppose we have 3 completely independent decision stump classifiers each with an accuracy of 70%.\n",
    "\n",
    "If we take a majority vote, what is the accuracy?\n",
    "\n",
    "What happens if we take the majority vote from 5 completely independent decision stump classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\binom{3}{2}(.7)^2(.3)^1 + \\binom{3}{3}(.7)^3(.3)^0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import operator as op\n",
    "from functools import reduce\n",
    "\n",
    "def n_choose_k(n, k):\n",
    "    k = min(k, n-k)\n",
    "    numerator   = reduce(op.mul, range(n, n-k, -1), 1)\n",
    "    denominator = reduce(op.mul, range(1, k+1), 1)\n",
    "    return numerator//denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.4%\n"
     ]
    }
   ],
   "source": [
    "majority_accuracy = ((n_choose_k(3, 2)*(.7**2)*(.3**1)) + \n",
    "                     (n_choose_k(3, 3)*(.7**3)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.4%\n"
     ]
    }
   ],
   "source": [
    "majority_accuracy = ((binom(3, 2)*(.7**2)*(.3**1)) + \n",
    "                     (binom(3, 3)*(.7**3)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we now have 5 completely independent decision stump classifiers each with an accuracy of 70%.\n",
    "\n",
    "What is the majority vote accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\binom{5}{3}(.7)^3(.3)^2 + \\binom{5}{4}(.7)^4(.3)^1 + \\binom{5}{5}(.7)^5(.3)^0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.7%\n"
     ]
    }
   ],
   "source": [
    "majority_accuracy = ((n_choose_k(5, 3)*(.7**3)*(.3**2)) + \n",
    "                     (n_choose_k(5, 4)*(.7**4)*(.3**1)) + \n",
    "                     (n_choose_k(5, 5)*(.7**5)*(.3**0)))\n",
    "print(f\"{majority_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we had 101 such classifiers, we would have 99.9% majority vote accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Voting can work with heterogeneous algorithms </h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and split the data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble       import VotingClassifier\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.svm            import SVC\n",
    "from sklearn.tree           import DecisionTreeClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "dt_clf  = DecisionTreeClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('knn', knn_clf), \n",
    "                                            ('dt', dt_clf), \n",
    "                                            ('svc',svm_clf)],\n",
    "                              voting = 'hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test) # 🍾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Types of Voting\n",
    "------\n",
    "\n",
    "__Hard voting__:  For classification, majority vote of individual learners (protip - pick an odd number to less the chance of ties). For regression, average predictions of individual learners.\n",
    "\n",
    "__Soft Voting__: Probability-weighted average of individual learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bagging, aka Bootstrap Aggregating\n",
    "----\n",
    "<br>\n",
    "<center><img src=\"https://www.oreilly.com/library/view/python-machine-learning/9781783555130/graphics/3547_07_06.jpg\" width=\"50%\"/></center>\n",
    "\n",
    "Fit multiple models in parallel and independently.\n",
    "\n",
    "Each model gets a vote on the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstrap Statistical Procedure\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\" width=\"75%\"/></center>\n",
    "\n",
    "Create many random subsamples with replacement. Compute the statistic of each subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstrap Sampling Steps\n",
    "-----\n",
    "\n",
    "1. Start with your dataset of size $n$\n",
    "1. Sample from your dataset with replacement to create 1 bootstrap sample of size $n$ which means many of the observations will be repeated\n",
    "1. Repeat $B$ times\n",
    "1. Each bootstrap sample can then be used as a separate dataset for model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bootstraping\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/image20-768x289.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bagging\n",
    "----\n",
    "\n",
    "<center><img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png\" width=\"55%\"/></center>\n",
    "\n",
    "You can bag with any collection of algorithms, giving them each a vote in the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Although usually applied to tree-based algorithms, it can be used with any type of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For regression problems (predicting a continuous value), we average the values given by all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For classification problems (predicting a categorical value), we choose the label with the most votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "\n",
    "# help(BaggingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(knn, \n",
    "                        max_samples=.5, \n",
    "                        max_features=2, \n",
    "                        oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),\n",
       "                  max_features=2, max_samples=0.5, oob_score=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "print(f\"{bag.oob_score_:,.2f}\") # Tends to overestimate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "score = bag.score(X_test, y_test)\n",
    "print(f\"{score:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do Bagging?\n",
    "------\n",
    "\n",
    "- Increases evaluation metric performance.\n",
    "- Less likely to overfitting since permutations of datasets are fit.\n",
    "- Improves stability of estimates. If ML people ever made error bars, they would be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stacking (not just for Transformers) \n",
    "-----\n",
    "\n",
    "An ensemble learning technique that uses predictions from previous models to build a new model. \n",
    "\n",
    "Two variations:\n",
    "\n",
    "1. Pipeline\n",
    "1. Metalearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Common Examples of Pipeline Stacking\n",
    "-----\n",
    "\n",
    "1. Build a search engine - High recall classifier ⇨ high precision classifier\n",
    "1. Rare event - yes/no classifier  ⇨ if yes, classify type of event (predicting unconditional type of event is too hard)\n",
    "1. Mixing supervised and unsupervised - clustering (or topic modeling)  ⇨ A separate classifier for each cluster. Different clusters may have different feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Stacking with a Metalearner\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://hsto.org/getpro/habr/post_images/c28/f6a/e29/c28f6ae298041c65eba7a97d3fbcce8e.png\" width=\"75%\"/></center>\n",
    "\n",
    "Metalearner learns the optimal combination of the base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "\n",
    "# help(StackingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors    import KNeighborsRegressor\n",
    "\n",
    "estimators = [('ridge', RidgeCV()),\n",
    "              ('lasso', LassoCV()),\n",
    "              ('knr',   KNeighborsRegressor(n_neighbors=20,\n",
    "                                            metric='euclidean'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "final_estimator = GradientBoostingRegressor(n_estimators=25, \n",
    "                                            subsample=0.5, \n",
    "                                            min_samples_leaf=25, \n",
    "                                            max_features=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = StackingRegressor(estimators=estimators,\n",
    "                        final_estimator=final_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 19.88\n"
     ]
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(f'mse: {mean_squared_error(y_test, y_pred):.2f}'.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Challenge Question\n",
    "-------\n",
    "\n",
    "If a data point is incorrectly predicted by the several of the models for a systematic reason, will combining the predictions provide better results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Answer__: No - We need a method to improve the errors across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boosting\n",
    "----\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png\" width=\"100%\"/></center>\n",
    "\n",
    "A sequential process, where each subsequent model attempts to correct the errors of the previous models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Boosting algorithms\n",
    "-----\n",
    "\n",
    "- XGBoost (Currently, the most popular)\n",
    "- AdaBoost\n",
    "- Gradient Boosting Machine (GBM) or Light GBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> ML 2 will cover boosting</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Bagging?\n",
    "------\n",
    "\n",
    "If you have time and enough data, bagging is a good choice because it only improves model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Stacking?\n",
    "------\n",
    "\n",
    "It is generally a good idea to pipe the outputs of one model into the inputs of another model.\n",
    "\n",
    "However, creating a meta-learner that is able to choose among heterogeneous models is often too complex for the typical gain in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When should you choose Boosting?\n",
    "------\n",
    "\n",
    "Boosting is an useful choice if highest level model performance on an evaluation metric is required. Boosting tends to win Kaggle contests. \n",
    "\n",
    "However, it is more complex than Bagging (harder to implement and harder to debug)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "How does Bagging partition the subsetted data?  \n",
    "How does Boosting partition the subsetted data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bagging does random partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Boosting samples data with errors at a higher preference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Ensembles are collections of model that will perform better than any individual model.\n",
    "- Stacking is chaining the output of one model as the input of another model.\n",
    "- Boosting is where subsequent models learn to correct the errors of earlier models.\n",
    "- Bagging is repeatedly sampling the training data and fitting a model to each resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Will Bagging increase or decrease Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Never increase Bias. Most of the time decrease Bias.\n",
    "\n",
    "The final prediction error is often lower than any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Will Bagging increase or decrease Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Never increase Variance. Most of the time decrease Variance.\n",
    "\n",
    "By combining predictions, they will not overfit to specific attributes of certain training sets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
