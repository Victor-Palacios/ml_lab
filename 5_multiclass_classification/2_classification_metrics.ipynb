{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classification-Evaluation-Metrics\" data-toc-modified-id=\"Classification-Evaluation-Metrics-1\">Classification Evaluation Metrics</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Why-should-we-care-about-evaluation-metrics?\" data-toc-modified-id=\"Why-should-we-care-about-evaluation-metrics?-3\">Why should we care about evaluation metrics?</a></span></li><li><span><a href=\"#What-are-common-evaluation-metrics-for-classification?\" data-toc-modified-id=\"What-are-common-evaluation-metrics-for-classification?-4\">What are common evaluation metrics for classification?</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-5\">Accuracy</a></span></li><li><span><a href=\"#What-are-the-biggest-limitations-of-accuracy?\" data-toc-modified-id=\"What-are-the-biggest-limitations-of-accuracy?-6\">What are the biggest limitations of accuracy?</a></span></li><li><span><a href=\"#Null-Accuracy\" data-toc-modified-id=\"Null-Accuracy-7\">Null Accuracy</a></span></li><li><span><a href=\"#Confusion-Matrix-(Brian's-♥️-Diagnostic-Tool)\" data-toc-modified-id=\"Confusion-Matrix-(Brian's-♥️-Diagnostic-Tool)-8\">Confusion Matrix (Brian's ♥️ Diagnostic Tool)</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-9\">Student Activity</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10\">Confusion Matrix</a></span></li><li><span><a href=\"#How-you'll-always-remember-the-difference\" data-toc-modified-id=\"How-you'll-always-remember-the-difference-11\">How you'll always remember the difference</a></span></li><li><span><a href=\"#Confusion-Matrix-for-Justice-System\" data-toc-modified-id=\"Confusion-Matrix-for-Justice-System-12\">Confusion Matrix for Justice System</a></span></li><li><span><a href=\"#Confusion-Matrix-for-Muliclass-Classification\" data-toc-modified-id=\"Confusion-Matrix-for-Muliclass-Classification-13\">Confusion Matrix for Muliclass Classification</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-14\">Check for understanding</a></span></li><li><span><a href=\"#Number-of-Classes-->-Number-of-Cells-\" data-toc-modified-id=\"Number-of-Classes-->-Number-of-Cells--15\">Number of Classes -&gt; Number of Cells </a></span></li><li><span><a href=\"#Precision-vs.-Recall\" data-toc-modified-id=\"Precision-vs.-Recall-16\">Precision vs. Recall</a></span></li><li><span><a href=\"#Classification---Getting-water-out-of-a-well-with-a-bucket\" data-toc-modified-id=\"Classification---Getting-water-out-of-a-well-with-a-bucket-17\">Classification - Getting water out of a well with a bucket</a></span></li><li><span><a href=\"#Precision-&amp;-Recall-for-Multiclass-Classification\" data-toc-modified-id=\"Precision-&amp;-Recall-for-Multiclass-Classification-18\">Precision &amp; Recall for Multiclass Classification</a></span></li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-19\">Recall</a></span></li><li><span><a href=\"#Questions-about-recall?\" data-toc-modified-id=\"Questions-about-recall?-20\">Questions about recall?</a></span></li><li><span><a href=\"#Precision-for-Multiclass-Classification\" data-toc-modified-id=\"Precision-for-Multiclass-Classification-21\">Precision for Multiclass Classification</a></span></li><li><span><a href=\"#Precision\" data-toc-modified-id=\"Precision-22\">Precision</a></span></li><li><span><a href=\"#Questions-about-precision?\" data-toc-modified-id=\"Questions-about-precision?-23\">Questions about precision?</a></span></li><li><span><a href=\"#Check-for-understanding-#1\" data-toc-modified-id=\"Check-for-understanding-#1-24\">Check for understanding #1</a></span></li><li><span><a href=\"#Check-for-understanding-#2\" data-toc-modified-id=\"Check-for-understanding-#2-25\">Check for understanding #2</a></span></li><li><span><a href=\"#F1-score\" data-toc-modified-id=\"F1-score-26\">F<sub>1</sub> score</a></span></li><li><span><a href=\"#Which-model-is-better?\" data-toc-modified-id=\"Which-model-is-better?-27\">Which model is better?</a></span></li><li><span><a href=\"#Generalized-F-score\" data-toc-modified-id=\"Generalized-F-score-28\">Generalized F score</a></span></li><li><span><a href=\"#Wide-world-of-Classification-Metrics\" data-toc-modified-id=\"Wide-world-of-Classification-Metrics-29\">Wide world of Classification Metrics</a></span></li><li><span><a href=\"#Which-metrics-should-you-choose?\" data-toc-modified-id=\"Which-metrics-should-you-choose?-30\">Which metrics should you choose?</a></span></li><li><span><a href=\"#Triangle-of-Sadness-for-Advertising-Platforms\" data-toc-modified-id=\"Triangle-of-Sadness-for-Advertising-Platforms-31\">Triangle of Sadness for Advertising Platforms</a></span></li><li><span><a href=\"#airbnb-Custom-Pricing-Case-Study\" data-toc-modified-id=\"airbnb-Custom-Pricing-Case-Study-32\">airbnb Custom Pricing Case Study</a></span></li><li><span><a href=\"#Using-Metrics-to-Address-Fairness-in-Models\" data-toc-modified-id=\"Using-Metrics-to-Address-Fairness-in-Models-33\">Using Metrics to Address Fairness in Models</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-34\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-35\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-36\">Bonus Material</a></span></li><li><span><a href=\"#Why-should-I-learn-ROC?\" data-toc-modified-id=\"Why-should-I-learn-ROC?-37\">Why should I learn ROC?</a></span></li><li><span><a href=\"#AUC-is-only-for-Binary-Classifiers\" data-toc-modified-id=\"AUC-is-only-for-Binary-Classifiers-38\">AUC is only for Binary Classifiers</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification Evaluation Metrics</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"https://i.pinimg.com/736x/18/c0/36/18c036f262ef322194553462279e5bbf.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- List common classification metrics.\n",
    "- Explain the limitations of accuracy as a metric.\n",
    "- Construct a confusion matrix.\n",
    "- Define precision, recall, and F score.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why should we care about evaluation metrics?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluation metrics help select better, aka more useful, models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which algorithm should we choose?  \n",
    "Which hyperparameters are better?   \n",
    "Are these good parameter estimates?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are common evaluation metrics for classification?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy \n",
    "- Recall\n",
    "- Precision\n",
    "- F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Accuracy</h2></center>\n",
    "\n",
    "$$Accuracy = \\frac{All\\ Correct}{Total}$$\n",
    "\n",
    "- Fraction of observations classified correctly\n",
    "- 1 - error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the biggest limitations of accuracy?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Accuracy is an overall measure. It does not tell you what kind of errors the classifier is making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Accuracy is effected by class imbalances (more examples from one category than other categories).\n",
    "\n",
    "Accuracy paradox - A predictive model may have high accuracy but is useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced accuracy can help with class imbalances \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# help(balanced_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../images/accuracy_pie_chart.png\" width=\"110%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Null Accuracy</h2></center>\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/is_it_christmas.png\" width=\"35%\"/></center>\n",
    "\n",
    "Accuracy that could be achieved by always predicting the most frequent class.\n",
    "\n",
    "Also called baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What is the null accuracy in this case?\n",
    "n           = 10_000\n",
    "n_not_fraud = 9_952\n",
    "n_fraud     = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting 'not fraud' will result in 99.52% accuracy.\n"
     ]
    }
   ],
   "source": [
    "null_accuracy = (n - n_fraud)/n\n",
    "print(f\"Always predicting 'not fraud' will result in {null_accuracy:.2%} accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A null accuracy model with imbalanced data is correct almost all of the time but useless.\n",
    "\n",
    "Let's move beyond accuracy to more nuanced metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix (Brian's ♥️ Diagnostic Tool)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "<center>Complete this confusion matrix:</center>\n",
    "<br>\n",
    "<center><img src=\"../images/confusion_matrix_blank.png\" width=\"45%\"/></center>\n",
    "<center>{True Negatives, True Positives, False Negatives, False Positives}</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Confusion Matrix</h2></center>\n",
    "\n",
    "<center><img src=\"../images/confusion_matrix.png\" width=\"40%\"/></center>\n",
    "\n",
    "- `Positive` and `Negative` refer to the result - presence or absence.\n",
    "- `True` and `False` refer to whether the model was correct.\n",
    "- True Positives (TP): correctly predicted a successful outcome. \n",
    "- True Negatives (TN): correctly predicted a lack of an outcome.\n",
    "- False Positives (FP): incorrectly predicted a successful outcome (a \"Type I error\").\n",
    "- False Negatives (FN): incorrectly predicted lack of an outcome (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How you'll always remember the difference</h2></center>\n",
    "<center><img src=\"../images/pregnant.jpg\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix for Justice System</h2></center>\n",
    "\n",
    "<center><img src=\"../images/cm_justice.png\" width=\"75%\"/></center>\n",
    "\n",
    "You can set your prediction threshold for the type of mistake you are will to tolerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix for Muliclass Classification</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1, multi_class='multinomial', solver='newton-cg')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load handwritten, then digitized digits\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.images.reshape((len(digits.images), -1)) # Convert from a matrix to a vector\n",
    "y = digits.target\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', # Explicitly define the problem as multiclass\n",
    "                         solver='newton-cg',        # It is best practices to explicitly define your solver\n",
    "                         max_iter=1                 # Set this low so our model does not have good parameters\n",
    "                        )\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[177   0   0   0   1   0   0   0   0   0]\n",
      " [  0 137  14   1   1   0   9   4   0  16]\n",
      " [  1   5 162   5   0   0   0   4   0   0]\n",
      " [  1   0   2 166   0   2   1   9   0   2]\n",
      " [  1   0   0   0 172   0   0   8   0   0]\n",
      " [  1   0   0   0   2 159   3   1   0  16]\n",
      " [  1   1   0   0   1   0 178   0   0   0]\n",
      " [  0   0   1   0   0   0   0 178   0   0]\n",
      " [  1  31  11  11   2   4  12  17  67  18]\n",
      " [  1   2   0   4   6   4   0   9   0 154]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "How does confusion matrix scale as a function of the number of classes (k)?\n",
    "\n",
    "If there are 10 classes, how many cells does the confusion matrix have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Number of Classes -> Number of Cells </h2></center>\n",
    "\n",
    "If there are 2 classes, there are 4 cells.  \n",
    "If there are 3 classes, there are 9 cells.  \n",
    "If there are 4 classes, there are 16 cells.  \n",
    "…<br>     \n",
    "If there are 10 classes, there are 100 cells.  \n",
    "…<br>\n",
    "If there are 100 classes, there are 10,000 cells.  \n",
    "\n",
    "The number of cells in a confusion matrix scale as __k<sup>2</sup>__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision vs. Recall</h2></center>\n",
    "\n",
    "<center><img src=\"../images/confus1.png\" width=\"75%\"/></center>\n",
    "\n",
    "The same numerator for both precision and recall.\n",
    "\n",
    "The denominator only has a minor difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification - Getting water out of a well with a bucket</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/bucket.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall - You get all the water out of well (water is positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Precision - You have only water in the bucket (no mud / negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision & Recall for Multiclass Classification</h2></center>\n",
    "\n",
    "<center><img src=\"../images/p_r.png\" width=\"100%\"/></center>\n",
    "\n",
    "In multiclass classification and precision, recall can be calculated for each label independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall</h2></center>\n",
    "\n",
    "$$Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives} = \\frac{Class\\ Correct}{Class\\ Total\\ Actual}$$\n",
    "<br>\n",
    "<center>Fraction of actual labeled items that are classified correctly.</center>\n",
    "<center>Maximize recall to be more certain to find all positive examples.</center>\n",
    "<center>All things that are actually X are labeled by the classifier as X.</center>\n",
    "<center>We want a classifier to find (recall) positive examples.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_1 = [0] * 10\n",
    "\n",
    "# Take a momemt to answer these questions:\n",
    "# What is recall?\n",
    "# What is recall for class 1?\n",
    "# What is recall for class 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# scikit-learn's default is by postive class\n",
    "recall_score(y_true, y_pred_model_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Switch to other class\n",
    "recall_score(y_true, y_pred_model_1, pos_label=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 'macro': Calculate metrics for each label, and find their unweighted mean. \n",
    "# This does not take label imbalance into account.\n",
    "recall_score(y_true, y_pred_model_1, average='macro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's interpert another model\n",
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_2 = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 'weighted': Calculate metrics for each label, \n",
    "# and find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Weighted recall is the same as accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Questions about recall?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Precision for Multiclass Classification</h2></center>\n",
    "\n",
    "<center><img src=\"../images/p_r.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision</h2></center>\n",
    "\n",
    "$$Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} = \\frac{Class\\ Correct}{Class\\ Total\\ Predicted}$$\n",
    "<br>\n",
    "<center>Fraction of predicted labeled items assigned to a class that are actually members of that class.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Maximize precision to be more certain everything labeled as positive is positive.</center>\n",
    "<center>All things predicted as X are actually X.</center>\n",
    "<center>We want a classifier to <b>precisely predict</b> labels for positives items.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(f\"{precision_score(y_true, y_pred_model_1)} precision for 1\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_1, pos_label=0)} precision for 0\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "precision_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{precision_score(y_true, y_pred_model_2)} (2 out of 5) precision for 1\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_2, pos_label=0)} (5 out of 5) precision for 0.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Average weighted by support for each label\n",
    "precision_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Questions about precision?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding #1</h2></center>\n",
    "\n",
    "If I want to avoiding false-positives, I should maximize:\n",
    "\n",
    "- Recall\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "Precision - Avoid predicting a positive result for a truly negative instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding #2</h2></center>\n",
    "\n",
    "I'm running a distributed computing system. I think one of the computers is slightly broken (called a gray failure). I should maximize:\n",
    "\n",
    "- Recall\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "Recall - I want to make sure to find it. It is okay if I happen to look at non-broken computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>F<sub>1</sub> score</h2></center>\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision+Recall} $$\n",
    "\n",
    "A single metric that combines precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_true, y_pred_model_1, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f1_score(y_true, y_pred_model_2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Which model is better?</h2></center>\n",
    "\n",
    "It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Generalized F score</h2></center>\n",
    "\n",
    "<center><img src=\"../images/f_score_2.png\" width=\"75%\"/></center>\n",
    "\n",
    "$F_1$ weighs recall and precision equally.\n",
    "\n",
    "$F_{β<1}$ weighs recall lower than precision (by reducing the influence of false negatives). $F_{0.5}$  is a common choice.\n",
    "\n",
    "$F_{β>1}$ weighs recall higher than precision (by placing more emphasis on false negatives). $F_{2.0}$  is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Wide world of Classification Metrics</h2></center>\n",
    "\n",
    "[Wikipedia's Classification Metrics](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Which metrics should you choose?</h2></center>\n",
    "\n",
    "<center>Choice of metric depends on your business goals.</center>\n",
    "<center>You should define custom evaluation metrics.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Triangle of Sadness for Advertising Platforms</h2></center>\n",
    "\n",
    "<center><img src=\"https://business.midco.com/globalassets/services/advertising-services-2019/digital-advertising-icon.png\" width=\"75%\"/></center>\n",
    "\n",
    "- People want precision in ads.\n",
    "- Companies want recall in ads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>airbnb Custom Pricing Case Study</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"../images/table.png\" width=\"80%\"/></center>\n",
    "\n",
    "<center>Pricing Sugg(ested) threshold by actual Price</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../images/custom.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://www.kdd.org/kdd2018/accepted-papers/view/customized-regression-model-for-airbnb-dynamic-pricing\n",
    "- https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Using Metrics to Address Fairness in Models</h2></center>\n",
    "\n",
    "[Example here](https://dssg.github.io/aequitas/examples/compas_demo.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://dssg.github.io/aequitas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- The most common classification metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F-score\n",
    "- Accuracy is not accurate if there are class imbalances. In real DS, there are almost always class imbalances.\n",
    "- A confusion matrix is an awesome way to visualize error types.\n",
    "- Brian ♥️s confusion matrices so always bring him one when asking for advice.\n",
    "- Precision measures if a classifier has right labels.\n",
    "- Recall measures if a classifier does not miss a label.\n",
    "- F score combines precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "-     https://stats.stackexchange.com/questions/117654/what-does-the-numbers-in-the-classification-report-of-sklearn-mean\n",
    "- https://datascience.stackexchange.com/questions/86555/machine-learning-precision-and-recall-differences-in-interpretation-and-pref\n",
    "- [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why should I learn ROC?</h2></center>\n",
    "\n",
    "1. Common interview question - It is still part of the cannon for data science.\n",
    "\n",
    "2. Work in a medical field - Medical field, especially medical research, still uses it.\n",
    "\n",
    "3. Education others - Some people still default to it. You can help teach them better ways but first you must understand their worldview.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>AUC is only for Binary Classifiers</h2></center>\n",
    "\n",
    "AUC is (area under curve) is only for binary Classifiers.\n",
    "\n",
    "Tither truly binary problem or multi-class that is framed one-vs-rest or one-vs-one vs.\n",
    "\n",
    "AUC does not scale to a truly multi-class problem. It would be hyper-volumne-under-curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-label classification evaluation metrics](https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
