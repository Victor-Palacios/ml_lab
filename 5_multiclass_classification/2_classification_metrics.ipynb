{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classification-Evaluation-Metrics\" data-toc-modified-id=\"Classification-Evaluation-Metrics-1\">Classification Evaluation Metrics</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Why-should-we-care-about-evaluation-metrics?\" data-toc-modified-id=\"Why-should-we-care-about-evaluation-metrics?-3\">Why should we care about evaluation metrics?</a></span></li><li><span><a href=\"#What-are-common-evaluation-metrics-for-classification?\" data-toc-modified-id=\"What-are-common-evaluation-metrics-for-classification?-4\">What are common evaluation metrics for classification?</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-5\">Accuracy</a></span></li><li><span><a href=\"#What-are-the-biggest-limitations-of-accuracy?\" data-toc-modified-id=\"What-are-the-biggest-limitations-of-accuracy?-6\">What are the biggest limitations of accuracy?</a></span></li><li><span><a href=\"#Null-Accuracy\" data-toc-modified-id=\"Null-Accuracy-7\">Null Accuracy</a></span></li><li><span><a href=\"#Confusion-Matrix-(Brian's-♥️-Diagnostic-Tool)\" data-toc-modified-id=\"Confusion-Matrix-(Brian's-♥️-Diagnostic-Tool)-8\">Confusion Matrix (Brian's ♥️ Diagnostic Tool)</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-9\">Student Activity</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10\">Confusion Matrix</a></span></li><li><span><a href=\"#How-you'll-always-remember-the-difference\" data-toc-modified-id=\"How-you'll-always-remember-the-difference-11\">How you'll always remember the difference</a></span></li><li><span><a href=\"#Confusion-Matrix-for-Justice-System\" data-toc-modified-id=\"Confusion-Matrix-for-Justice-System-12\">Confusion Matrix for Justice System</a></span></li><li><span><a href=\"#Confusion-Matrix-for-Muliclass-Classification\" data-toc-modified-id=\"Confusion-Matrix-for-Muliclass-Classification-13\">Confusion Matrix for Muliclass Classification</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-14\">Check for understanding</a></span></li><li><span><a href=\"#Number-of-Classes-->-Number-of-Cells-\" data-toc-modified-id=\"Number-of-Classes-->-Number-of-Cells--15\">Number of Classes -&gt; Number of Cells </a></span></li><li><span><a href=\"#Precision-vs.-Recall\" data-toc-modified-id=\"Precision-vs.-Recall-16\">Precision vs. Recall</a></span></li><li><span><a href=\"#Classification---Getting-water-out-of-a-well-with-a-bucket\" data-toc-modified-id=\"Classification---Getting-water-out-of-a-well-with-a-bucket-17\">Classification - Getting water out of a well with a bucket</a></span></li><li><span><a href=\"#Precision-&amp;-Recall-for-Multiclass-Classification\" data-toc-modified-id=\"Precision-&amp;-Recall-for-Multiclass-Classification-18\">Precision &amp; Recall for Multiclass Classification</a></span></li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-19\">Recall</a></span></li><li><span><a href=\"#Questions-about-recall?\" data-toc-modified-id=\"Questions-about-recall?-20\">Questions about recall?</a></span></li><li><span><a href=\"#Precision-for-Multiclass-Classification\" data-toc-modified-id=\"Precision-for-Multiclass-Classification-21\">Precision for Multiclass Classification</a></span></li><li><span><a href=\"#Precision\" data-toc-modified-id=\"Precision-22\">Precision</a></span></li><li><span><a href=\"#Questions-about-precision?\" data-toc-modified-id=\"Questions-about-precision?-23\">Questions about precision?</a></span></li><li><span><a href=\"#Check-for-understanding-#1\" data-toc-modified-id=\"Check-for-understanding-#1-24\">Check for understanding #1</a></span></li><li><span><a href=\"#Check-for-understanding-#2\" data-toc-modified-id=\"Check-for-understanding-#2-25\">Check for understanding #2</a></span></li><li><span><a href=\"#F1-score\" data-toc-modified-id=\"F1-score-26\">F<sub>1</sub> score</a></span></li><li><span><a href=\"#Which-model-is-better?\" data-toc-modified-id=\"Which-model-is-better?-27\">Which model is better?</a></span></li><li><span><a href=\"#Generalized-F-score\" data-toc-modified-id=\"Generalized-F-score-28\">Generalized F score</a></span></li><li><span><a href=\"#Wide-World-of-Classification-Metrics\" data-toc-modified-id=\"Wide-World-of-Classification-Metrics-29\">Wide World of Classification Metrics</a></span></li><li><span><a href=\"#Which-metrics-should-you-choose?\" data-toc-modified-id=\"Which-metrics-should-you-choose?-30\">Which metrics should you choose?</a></span></li><li><span><a href=\"#Triangle-of-Sadness-for-Advertising-Platforms\" data-toc-modified-id=\"Triangle-of-Sadness-for-Advertising-Platforms-31\">Triangle of Sadness for Advertising Platforms</a></span></li><li><span><a href=\"#airbnb-Custom-Pricing-Case-Study\" data-toc-modified-id=\"airbnb-Custom-Pricing-Case-Study-32\">airbnb Custom Pricing Case Study</a></span></li><li><span><a href=\"#Using-Metrics-to-Address-Fairness-in-Models\" data-toc-modified-id=\"Using-Metrics-to-Address-Fairness-in-Models-33\">Using Metrics to Address Fairness in Models</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-34\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-35\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-36\">Bonus Material</a></span></li><li><span><a href=\"#Why-should-I-learn-ROC?\" data-toc-modified-id=\"Why-should-I-learn-ROC?-37\">Why should I learn ROC?</a></span></li><li><span><a href=\"#AUC-is-only-for-Binary-Classifiers\" data-toc-modified-id=\"AUC-is-only-for-Binary-Classifiers-38\">AUC is only for Binary Classifiers</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification Evaluation Metrics</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"https://i.pinimg.com/736x/18/c0/36/18c036f262ef322194553462279e5bbf.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- List common classification metrics.\n",
    "- Explain the limitations of accuracy as a metric.\n",
    "- Construct a confusion matrix.\n",
    "- Define precision, recall, and F score.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why should we care about evaluation metrics?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluation metrics help us select more useful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which algorithm should we choose? Is algorithm A better than algorithm B?\n",
    "Which hyperparameters are better?   \n",
    "Are these good parameter estimates?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are common evaluation metrics for classification?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy \n",
    "- Recall\n",
    "- Precision\n",
    "- F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Accuracy</h2></center>\n",
    "\n",
    "$$Accuracy = \\frac{All\\ Correct}{Total}$$\n",
    "\n",
    "- Fraction of observations classified correctly\n",
    "- 1 - error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the biggest limitations of accuracy?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Accuracy is an overall measure. It does not tell you what kind of errors the classifier is making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Accuracy is effected by class imbalances (more examples from one category than other categories).\n",
    "\n",
    "Accuracy paradox - A predictive model may have high accuracy but is useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced accuracy can help with class imbalances \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# help(balanced_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../images/accuracy_pie_chart.png\" width=\"110%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Null Accuracy</h2></center>\n",
    "\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/is_it_christmas.png\" width=\"35%\"/></center>\n",
    "\n",
    "Accuracy that could be achieved by always predicting the most frequent class.\n",
    "\n",
    "Also called baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What is the null accuracy in this case?\n",
    "n           = 10_000\n",
    "n_not_fraud = 9_952\n",
    "n_fraud     = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting 'not fraud' will result in 99.52% accuracy.\n"
     ]
    }
   ],
   "source": [
    "null_accuracy = (n - n_fraud)/n\n",
    "print(f\"Always predicting 'not fraud' will result in {null_accuracy:.2%} accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A null accuracy model with imbalanced data is correct almost all of the time but useless.\n",
    "\n",
    "Let's move beyond accuracy to more nuanced metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix (Brian's ♥️ Diagnostic Tool)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "<center>Complete this confusion matrix:</center>\n",
    "<br>\n",
    "<center><img src=\"../images/confusion_matrix_blank.png\" width=\"45%\"/></center>\n",
    "<center>{True Negatives, True Positives, False Negatives, False Positives}</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Confusion Matrix</h2></center>\n",
    "\n",
    "<center><img src=\"../images/confusion_matrix.png\" width=\"40%\"/></center>\n",
    "\n",
    "- `Positive` and `Negative` refer to the result - presence or absence.\n",
    "- `True` and `False` refer to whether the model was correct.\n",
    "- True Positives (TP): correctly predicted a successful outcome. \n",
    "- True Negatives (TN): correctly predicted a lack of an outcome.\n",
    "- False Positives (FP): incorrectly predicted a successful outcome (a \"Type I error\").\n",
    "- False Negatives (FN): incorrectly predicted lack of an outcome (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How you'll always remember the difference</h2></center> \n",
    "<br>\n",
    "<center><img src=\"../images/pregnant.jpg\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix for Justice System</h2></center>\n",
    "\n",
    "<center><img src=\"../images/cm_justice2.png\" width=\"75%\"/></center>\n",
    "\n",
    "You can set your prediction threshold for the type of mistake you are will to tolerate.\n",
    "\n",
    "In the United States, we do not want to convict innocent people so the threshold is \"beyond a reasonable doubt\".\n",
    "\n",
    "It is more okay to let someone who actual did a crime, go unpunished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Confusion Matrix for Muliclass Classification</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/optimize.py:211: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1, multi_class='multinomial', solver='newton-cg')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load handwritten, then digitized digits\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.images.reshape((len(digits.images), -1)) # Convert from a matrix to a vector\n",
    "y = digits.target\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', # Explicitly define the problem as multiclass\n",
    "                         solver='newton-cg',        # It is best practices to explicitly define your solver\n",
    "                         max_iter=1                 # Set this low so our model does not have good parameters\n",
    "                        )\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[177,   0,   0,   0,   1,   0,   0,   0,   0,   0],\n",
       "       [  0, 137,  14,   1,   1,   0,   9,   4,   0,  16],\n",
       "       [  1,   5, 162,   5,   0,   0,   0,   4,   0,   0],\n",
       "       [  1,   0,   2, 166,   0,   2,   1,   9,   0,   2],\n",
       "       [  1,   0,   0,   0, 172,   0,   0,   8,   0,   0],\n",
       "       [  1,   0,   0,   0,   2, 159,   3,   1,   0,  16],\n",
       "       [  1,   1,   0,   0,   1,   0, 178,   0,   0,   0],\n",
       "       [  0,   0,   1,   0,   0,   0,   0, 178,   0,   0],\n",
       "       [  1,  31,  11,  11,   2,   4,  12,  17,  67,  18],\n",
       "       [  1,   2,   0,   4,   6,   4,   0,   9,   0, 154]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "confusion_matrix(y_true=y, y_pred=y_pred, labels=load_digits().target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's confusion matrix\n",
    "\n",
    "- Rows are the true class label.\n",
    "- Columns are predicted class label.\n",
    "\n",
    "Note: Other places might do it differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7   8    9\n",
       "0  177    0    0    0    1    0    0    0   0    0\n",
       "1    0  137   14    1    1    0    9    4   0   16\n",
       "2    1    5  162    5    0    0    0    4   0    0\n",
       "3    1    0    2  166    0    2    1    9   0    2\n",
       "4    1    0    0    0  172    0    0    8   0    0\n",
       "5    1    0    0    0    2  159    3    1   0   16\n",
       "6    1    1    0    0    1    0  178    0   0    0\n",
       "7    0    0    1    0    0    0    0  178   0    0\n",
       "8    1   31   11   11    2    4   12   17  67   18\n",
       "9    1    2    0    4    6    4    0    9   0  154"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print with labels\n",
    "import pandas as pd \n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_true=y, y_pred=y_pred, labels=load_digits().target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://stackoverflow.com/questions/50325786/sci-kit-learn-how-to-print-labels-for-confusion-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "How does confusion matrix scale as a function of the number of classes (k)?\n",
    "\n",
    "If there are 10 classes, how many cells does the confusion matrix have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Number of Classes -> Number of Cells </h2></center>\n",
    "\n",
    "If there are 2 classes, there are 4 cells.  \n",
    "If there are 3 classes, there are 9 cells.  \n",
    "If there are 4 classes, there are 16 cells.  \n",
    "…<br>     \n",
    "If there are 10 classes, there are 100 cells.  \n",
    "…<br>\n",
    "If there are 100 classes, there are 10,000 cells.  \n",
    "\n",
    "The number of cells in a confusion matrix scale as __k<sup>2</sup>__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision vs. Recall</h2></center>\n",
    "\n",
    "<center><img src=\"../images/confus1.png\" width=\"75%\"/></center>\n",
    "\n",
    "The same numerator for both precision and recall.\n",
    "\n",
    "The denominator only has a minor difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification - Getting water out of a well with a bucket</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/bucket.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Water is \"positive\" and mud is \"negative\".\n",
    "\n",
    "We want as much water as possible with as little mud.\n",
    "\n",
    "Recall - You get as much water as possible out of the well.\n",
    "\n",
    "Precision - You have has much water as possible in the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision & Recall for Multiclass Classification</h2></center>\n",
    "\n",
    "<center><img src=\"../images/p_r.png\" width=\"100%\"/></center>\n",
    "\n",
    "In multiclass classification and precision, recall can be calculated for each label independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall</h2></center>\n",
    "\n",
    "$$Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives} = \\frac{Class\\ Correct}{Class\\ Total\\ Actual}$$\n",
    "<br>\n",
    "<center>Fraction of actual labeled items that are classified correctly.</center>\n",
    "<center>Maximize recall to be more certain to find all positive examples.</center>\n",
    "<center>All things that are actually X are labeled by the classifier as X.</center>\n",
    "<center>We want a classifier to find (recall) positive examples.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_1 = [0] * 10\n",
    "\n",
    "# Take a momemt to answer these questions:\n",
    "# What is recall?\n",
    "# What is recall for class 1?\n",
    "# What is recall for class 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# scikit-learn's default is by postive class\n",
    "recall_score(y_true, y_pred_model_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to other class\n",
    "recall_score(y_true, y_pred_model_1, pos_label=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'macro': Calculate metrics for each label, and find their unweighted mean. \n",
    "# This does not take label imbalance into account.\n",
    "recall_score(y_true, y_pred_model_1, average='macro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's interpert another model\n",
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_2 = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 'weighted': Calculate metrics for each label, \n",
    "# and find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighted recall is the same as accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true, y_pred_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Questions about recall?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Precision for Multiclass Classification</h2></center>\n",
    "\n",
    "<center><img src=\"../images/p_r.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision</h2></center>\n",
    "\n",
    "$$Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} = \\frac{Class\\ Correct}{Class\\ Total\\ Predicted}$$\n",
    "<br>\n",
    "<center>Fraction of predicted labeled items assigned to a class that are actually members of that class.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Maximize precision to be more certain everything labeled as positive is positive.</center>\n",
    "<center>All things predicted as X are actually X.</center>\n",
    "<center>We want a classifier to <b>precisely predict</b> labels for positives items.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 precision for 1\n",
      "0.8 precision for 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(f\"{precision_score(y_true, y_pred_model_1)} precision for 1\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_1, pos_label=0)} precision for 0\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "precision_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 (2 out of 5) precision for 1\n",
      "1.0 (5 out of 5) precision for 0.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{precision_score(y_true, y_pred_model_2)} (2 out of 5) precision for 1\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_2, pos_label=0)} (5 out of 5) precision for 0.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800000000000001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average weighted by support for each label\n",
    "precision_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Questions about precision?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding #1</h2></center>\n",
    "\n",
    "If I want to avoiding false-positives, I should maximize:\n",
    "\n",
    "- Recall\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "Precision - Avoid predicting a positive result for a truly negative instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding #2</h2></center>\n",
    "\n",
    "I'm running a distributed computing system. I think one of the computers is slightly broken (called a gray failure). I should maximize:\n",
    "\n",
    "- Recall\n",
    "- Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__\n",
    "\n",
    "Recall - I want to make sure to find the broken computers. It is okay if I happen to look at non-broken computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>F<sub>1</sub> score</h2></center>\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision+Recall} $$\n",
    "\n",
    "A single metric that combines precision and recall.\n",
    "\n",
    "$ F_1$ equally weights precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_true, y_pred_model_1, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296703296703297\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_true, y_pred_model_2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.40      0.50      0.44        10\n",
      "weighted avg       0.64      0.80      0.71        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred_model_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.40      1.00      0.57         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.81      0.67        10\n",
      "weighted avg       0.88      0.70      0.73        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Which model is better?</h2></center>\n",
    "\n",
    "It depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Generalized F score</h2></center>\n",
    "\n",
    "<center><img src=\"../images/f_score_2.png\" width=\"75%\"/></center>\n",
    "\n",
    "$F_1$ weighs recall and precision equally.\n",
    "\n",
    "$F_{β<1}$ weighs recall lower than precision (by reducing the influence of false negatives). $F_{0.5}$  is a common choice.\n",
    "\n",
    "$F_{β>1}$ weighs recall higher than precision (by placing more emphasis on false negatives). $F_{2.0}$  is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Wide World of Classification Metrics</h2></center>\n",
    "\n",
    "[Wikipedia's Classification Metrics](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Which metrics should you choose?</h2></center>\n",
    "\n",
    "<center>Choice of metric depends on your business goals.</center>\n",
    "<center>You should define custom evaluation metrics.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Triangle of Sadness for Advertising Platforms</h2></center>\n",
    "\n",
    "<center><img src=\"https://business.midco.com/globalassets/services/advertising-services-2019/digital-advertising-icon.png\" width=\"75%\"/></center>\n",
    "\n",
    "Most platforms sell ads (e.g., Google, Facebook, LinkedIn, Yelp, …).\n",
    "\n",
    "In an ideal world, ads would have both high precision and high recall. All revelant ads would be shown (high recall) and only relevant ads would be shown (high precision). \n",
    "\n",
    "However, the platform has to choose between the two options and make someone sad. People (aka, eyeballs) want precision in ads. Companies that run marketing campaigns want recall in ads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>airbnb Custom Pricing Case Study</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"../images/table.png\" width=\"80%\"/></center>\n",
    "\n",
    "<center>Pricing Sugg(ested) threshold by actual Price</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"../images/custom.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://www.kdd.org/kdd2018/accepted-papers/view/customized-regression-model-for-airbnb-dynamic-pricing\n",
    "- https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Using Metrics to Address Fairness in Models</h2></center>\n",
    "\n",
    "[Examples here](https://dssg.github.io/aequitas/examples/compas_demo.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://dssg.github.io/aequitas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- The most common classification metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F-score\n",
    "- Accuracy is not accurate if there are class imbalances. In real DS, there are almost always class imbalances.\n",
    "- A confusion matrix is an awesome way to visualize error types.\n",
    "- Brian ♥️s confusion matrices so always bring him one when asking for advice.\n",
    "- Precision measures if a classifier has right labels.\n",
    "- Recall measures if a classifier does not miss a label.\n",
    "- F score combines precision and recall.\n",
    "- Use the principles to pick the correct metric for the problem. Also, define custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "-     https://stats.stackexchange.com/questions/117654/what-does-the-numbers-in-the-classification-report-of-sklearn-mean\n",
    "- https://datascience.stackexchange.com/questions/86555/machine-learning-precision-and-recall-differences-in-interpretation-and-pref\n",
    "- [The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why should I learn ROC?</h2></center>\n",
    "\n",
    "1. Common interview question - It is still part of the cannon for data science.\n",
    "\n",
    "2. Working in a medical field - The medical field, especially medical research, still uses it.\n",
    "\n",
    "3. Educating others - Some people still default to ROC. You can help teach them better ways but first you must understand their worldview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>AUC is only for Binary Classifiers</h2></center>\n",
    "\n",
    "AUC is (area under curve) is only for binary Classifiers.\n",
    "\n",
    "Tither truly binary problem or multi-class that is framed one-vs-rest or one-vs-one vs.\n",
    "\n",
    "AUC does not scale to a truly multi-class problem. It would be hyper-volumne-under-curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-label classification evaluation metrics](https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
