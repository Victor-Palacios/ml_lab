{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-1\">Hyperparameter Tuning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#-Machine-Learning-Training\" data-toc-modified-id=\"-Machine-Learning-Training-3\"> Machine Learning Training</a></span></li><li><span><a href=\"#ML-is-&quot;double-loop&quot;-optimization\" data-toc-modified-id=\"ML-is-&quot;double-loop&quot;-optimization-4\">ML is \"double loop\" optimization</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-5\">Check for understanding</a></span></li><li><span><a href=\"#What-are-parameter-search-methods?\" data-toc-modified-id=\"What-are-parameter-search-methods?-6\">What are parameter search methods?</a></span></li><li><span><a href=\"#Manual-Search\" data-toc-modified-id=\"Manual-Search-7\">Manual Search</a></span></li><li><span><a href=\"#Life-is-too-short-for-Manual-Search\" data-toc-modified-id=\"Life-is-too-short-for-Manual-Search-8\">Life is too short for Manual Search</a></span></li><li><span><a href=\"#Let's-automate-Manual-Search!\" data-toc-modified-id=\"Let's-automate-Manual-Search!-9\">Let's automate Manual Search!</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-10\">Grid Search</a></span></li><li><span><a href=\"#Grid-Search,-aka-just-try-everything!\" data-toc-modified-id=\"Grid-Search,-aka-just-try-everything!-11\">Grid Search, aka just try everything!</a></span></li><li><span><a href=\"#Cartesian-Product\" data-toc-modified-id=\"Cartesian-Product-12\">Cartesian Product</a></span></li><li><span><a href=\"#What-is-the-&quot;Curse-of-Dimensionality&quot;?\" data-toc-modified-id=\"What-is-the-&quot;Curse-of-Dimensionality&quot;?-13\">What is the \"Curse of Dimensionality\"?</a></span></li><li><span><a href=\"#Why-is-&quot;Curse-of-Dimensionality&quot;-an-issue-for-Grid-Search?\" data-toc-modified-id=\"Why-is-&quot;Curse-of-Dimensionality&quot;-an-issue-for-Grid-Search?-14\">Why is \"Curse of Dimensionality\" an issue for Grid Search?</a></span></li><li><span><a href=\"#&quot;Curse-of-dimensionality&quot;-example\" data-toc-modified-id=\"&quot;Curse-of-dimensionality&quot;-example-15\">\"Curse of dimensionality\" example</a></span></li><li><span><a href=\"#Enumerating-Grid-Search\" data-toc-modified-id=\"Enumerating-Grid-Search-16\">Enumerating Grid Search</a></span></li><li><span><a href=\"#Logistic-Regression-Hyperparameter-Serach:-Categorical\" data-toc-modified-id=\"Logistic-Regression-Hyperparameter-Serach:-Categorical-17\">Logistic Regression Hyperparameter Serach: Categorical</a></span></li><li><span><a href=\"#Numeric\" data-toc-modified-id=\"Numeric-18\">Numeric</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-19\">Check for understanding</a></span></li><li><span><a href=\"#Logistic-Regression-Hyperparameter-Serach:-Numeric\" data-toc-modified-id=\"Logistic-Regression-Hyperparameter-Serach:-Numeric-20\">Logistic Regression Hyperparameter Serach: Numeric</a></span></li><li><span><a href=\"#Grid-Search-with-Logistic-Regression-on-Iris-dataset\" data-toc-modified-id=\"Grid-Search-with-Logistic-Regression-on-Iris-dataset-21\">Grid Search with Logistic Regression on Iris dataset</a></span></li><li><span><a href=\"#Grid-vs-Random-Search\" data-toc-modified-id=\"Grid-vs-Random-Search-22\">Grid vs Random Search</a></span></li><li><span><a href=\"#-Random-Search->-Grid-Search:-Both-speed-and-performance\" data-toc-modified-id=\"-Random-Search->-Grid-Search:-Both-speed-and-performance-23\"> Random Search &gt; Grid Search: Both speed and performance</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-24\">Check for understanding</a></span></li><li><span><a href=\"#Hyperparameter-Search-Mind-Map\" data-toc-modified-id=\"Hyperparameter-Search-Mind-Map-25\">Hyperparameter Search Mind Map</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-26\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-27\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-28\">Bonus Material</a></span></li><li><span><a href=\"#Halving-search-to-speed-up-search\" data-toc-modified-id=\"Halving-search-to-speed-up-search-29\">Halving search to speed up search</a></span></li><li><span><a href=\"#1st-Order-Optimization-Review\" data-toc-modified-id=\"1st-Order-Optimization-Review-30\">1<sup>st</sup> Order Optimization Review</a></span></li><li><span><a href=\"#Calculus-Refresher\" data-toc-modified-id=\"Calculus-Refresher-31\">Calculus Refresher</a></span></li><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-32\">Loss Function</a></span></li><li><span><a href=\"#1st-Order-Optimization\" data-toc-modified-id=\"1st-Order-Optimization-33\">1st Order Optimization</a></span></li><li><span><a href=\"#2nd-Order-Optimization\" data-toc-modified-id=\"2nd-Order-Optimization-34\">2nd Order Optimization</a></span></li><li><span><a href=\"#1st-vs.-2nd-Order-Optimization\" data-toc-modified-id=\"1st-vs.-2nd-Order-Optimization-35\">1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</a></span></li><li><span><a href=\"#1st-vs.-2nd-Order-Optimization\" data-toc-modified-id=\"1st-vs.-2nd-Order-Optimization-36\">1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</a></span></li><li><span><a href=\"#2nd-order-Optimizers-Limitations\" data-toc-modified-id=\"2nd-order-Optimizers-Limitations-37\">2nd order Optimizers Limitations</a></span></li><li><span><a href=\"#lbfgs-Solver-for-Logistic-Regression\" data-toc-modified-id=\"lbfgs-Solver-for-Logistic-Regression-38\">lbfgs Solver for Logistic Regression</a></span></li><li><span><a href=\"#lbfgs-Solver-for-Logistic-Regression\" data-toc-modified-id=\"lbfgs-Solver-for-Logistic-Regression-39\">lbfgs Solver for Logistic Regression</a></span></li><li><span><a href=\"#Gradient-based-optimization-for-hyperparameter\" data-toc-modified-id=\"Gradient-based-optimization-for-hyperparameter-40\">Gradient-based optimization for hyperparameter</a></span></li><li><span><a href=\"#Hyperband\" data-toc-modified-id=\"Hyperband-41\">Hyperband</a></span></li><li><span><a href=\"#Bayesian-Optimization\" data-toc-modified-id=\"Bayesian-Optimization-42\">Bayesian Optimization</a></span></li><li><span><a href=\"#Tree-structured-Parzen-estimators-(TPE)\" data-toc-modified-id=\"Tree-structured-Parzen-estimators-(TPE)-43\">Tree-structured Parzen estimators (TPE)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hyperparameter Tuning</h2></center>\n",
    "\n",
    "<center><img src=\"../images/yoel-post-image-4.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define the \"double loop\" optimization problem in Machine Learning.\n",
    "- Compare and contrast 1st and 2nd order methods for parameter search.\n",
    "- Describe methods for hyperparameter search:\n",
    "    + Manual Search\n",
    "    + Grid Search\n",
    "    + Random Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Machine Learning Training</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "$$Features + Algorithm + Hyperparameters = Model\\ Parameters$$\n",
    "\n",
    "I will use every technique available to get better model:\n",
    "\n",
    "- More and better raw data.\n",
    "- More and better features.\n",
    "- Try every algorithm.\n",
    "- Try to find the best hyperparameters for the algorithm.\n",
    "- Train the model effectively to get the best model parameters.\n",
    "\n",
    "Some are easier (and cheaper) than other. Data (aka, the real world) tends to be expensive. Computer time is so cheap it almost free. Why not spend more of a cheap resource to get better models? Thus do good hyperparameter and parameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>ML is \"double loop\" optimization</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/m system simple.png\" width=\"60%\"/></center>\n",
    "\n",
    "<center>Inner loop is model training.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Outer loop is hyperparameter tuning.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Both loops are just search problems.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why is it difficult to tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically, it is difficult to find the best hyperparameters directly in terms of the training loss.\n",
    "\n",
    "We care about generalization performanc, via held-out data, thus each need to fit the model in order to the estimate a validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Learn more:\n",
    "\n",
    "- https://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/13-Optimization/04-secondOrderOpt.pdf\n",
    "- https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/\n",
    "- Random Search for Hyper-Parameter Optimization: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are parameter search methods?</h2></center>\n",
    "\n",
    "By parameter, I mean both model-parameters and hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Manual Search\n",
    "- Closed form solution (e.g., ordinary least squares (OLS))\n",
    "- 1<sup>st</sup> order methods (e.g., gradient descent)\n",
    "- 2<sup>nd</sup>order methods (e.g., Newton's method)\n",
    "- Grid Search\n",
    "- Random Search\n",
    "- Bayesian Optimization\n",
    "- So many others…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If possible always choose a closed form solution, a closed form solution will be the fastest and with best guarantees of finding optimal solution.\n",
    "\n",
    "However, it not always possible. OLS is not applicable for very large datasets (large $n$ and/or $p$) or datasets where the inverse of $X^TX$ does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Manual Search</h2></center>\n",
    "\n",
    "<center><img src=\"../images/phd-front.jpg\" width=\"45%\"/></center>\n",
    "\n",
    "- aka, GSD: Graduate Student Descent\n",
    "\n",
    "- 100% manual\n",
    "\n",
    "- Trial & (mostly) Error\n",
    "\n",
    "- The most common approach by amateurs, students and researchers.\n",
    "\n",
    "- Would you manually search for model-parameters? If not, then why would you manually search for hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Life is too short for Manual Search</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Let's automate Manual Search!</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search</h2></center>\n",
    "\n",
    "<center><img src=\"../images/battleship.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search, aka just try everything!</h2></center>\n",
    "\n",
    "Define a grid:\n",
    "\n",
    "1. Define each hyperparameter as a dimension.\n",
    "1. For each dimension, define the range of possible values.\n",
    "1. Try each combination (the cells of the grid). \n",
    "1. At the end, choose the best combination of parameters measured on cross-validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Cartesian Product</h2></center>\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Cartesian_Product_qtl1.svg/1200px-Cartesian_Product_qtl1.svg.png\" width=\"45%\"/></center>\n",
    "\n",
    "The product of two sets that generates all possible ordered pairs.\n",
    "\n",
    "If you see Cartesian Product, run away because of computational complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the \"Curse of Dimensionality\"?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally - As dimensionality increases linearly, the volume of the space increases much faster.\n",
    "\n",
    "If you have fixed data, increasing dimensionality increase sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is \"Curse of Dimensionality\" an issue for Grid Search?</h2></center>\n",
    "\n",
    "The number of times you are required to evaluate your model grows exponentially with the number of hyperparameters you want to test.\n",
    "\n",
    "Typically, it is computationally intractable to search all relevant dimensions and values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>\"Curse of dimensionality\" example</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of Dims # of Points\n",
      "     1            4\n",
      "     2           16\n",
      "     3           64\n",
      "     4          256\n",
      "     5        1,024\n",
      "     6        4,096\n",
      "     7       16,384\n",
      "     8       65,536\n",
      "     9      262,144\n",
      "    10    1,048,576\n"
     ]
    }
   ],
   "source": [
    "values_for_each_dim = 4 # 2 or 3 or 4\n",
    "n_dims = 10\n",
    "\n",
    "print(f\"{'# of Dims':>10} {'# of Points':>10}\")\n",
    "for n_dimensions in range(1, n_dims+1):\n",
    "    total_values = values_for_each_dim**n_dimensions\n",
    "    print(f\"{n_dimensions:>6} {total_values:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Enumerating Grid Search</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. Categorical (Strings or Booleans)\n",
    "1. Numeric (Integers or Floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Logistic Regression Hyperparameter Serach: Categorical</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l2', 'none'] # 'l1' is not supported by lbfgs solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Numeric</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "Select a finite set of \"reasonable\" values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What happens if you select an \"unreasonable\" set of values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://i.imgflip.com/2ni2bl.jpg\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "[2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "# For integers use either `range` or `np.arrage`\n",
    "pure_python = range(2, 10, 2)\n",
    "numpy = np.arange(start=2, stop=10, step=2)\n",
    "print(list(pure_python))\n",
    "print(numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.5   4.375 5.25  6.125 7.   ]\n",
      "[3.16227766e+03 2.37137371e+04 1.77827941e+05 1.33352143e+06\n",
      " 1.00000000e+07]\n"
     ]
    }
   ],
   "source": [
    "# For non-integers use `np.linspace` or `np.logspace`\n",
    "print(np.linspace(start=3.5, stop=7, num=5))\n",
    "print(np.logspace(start=3.5, stop=7, num=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Logistic Regression Hyperparameter Serach: Numeric</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create regularization hyperparameter space\n",
    "# C is the inverse of regularization strength\n",
    "C = np.logspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C [1.00000000e+00 2.78255940e+00 7.74263683e+00 2.15443469e+01\n",
      " 5.99484250e+01 1.66810054e+02 4.64158883e+02 1.29154967e+03\n",
      " 3.59381366e+03 1.00000000e+04]\n",
      "penalty ['l2', 'none']\n"
     ]
    }
   ],
   "source": [
    "# Put together categorical and numeric\n",
    "hyperparameters = dict(C=C, \n",
    "                       penalty=penalty,\n",
    "                       )\n",
    "for k, v in hyperparameters.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How many cells / configurations are we testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "20 (10 x 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search with Logistic Regression on Iris dataset</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf_grid = GridSearchCV(LogisticRegression(), \n",
    "                        hyperparameters, \n",
    "                        cv=5, \n",
    "                        verbose=1)\n",
    "clf_grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy - 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The test set accuracy - {clf_grid.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: none\n",
      "Best C: 1.0\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', clf_grid.best_estimator_.get_params()['penalty'])\n",
    "print(f\"Best C: {clf_grid.best_estimator_.get_params()['C']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid vs Random Search</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/grid vs random.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Grid Search have to pick specific values.\n",
    "- Random Search can define a distribution, then sample from distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Random Search > Grid Search: Both speed and performance</h2></center>\n",
    "\n",
    "<center><img src=\"../images/random_preformance.png\" width=\"50%\"/></center>\n",
    "\n",
    "<center>Blue line is Grid Search accuracy for 100 trials.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image sources](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=LogisticRegression(), \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=13, \n",
    "                              cv=5, \n",
    "                              verbose=1)\n",
    "clf_rand.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy - 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The test set accuracy - {clf_rand.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: none\n",
      "Best C: 464.16\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', clf_rand.best_estimator_.get_params()['penalty'])\n",
    "print(f\"Best C: {clf_rand.best_estimator_.get_params()['C']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "When should you use Grid Search?\n",
    "\n",
    "When should you use Random Search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your default should be Random Search. Hyperparameter is often so large and the process is so slow that Random Search is good heuristic approach.\n",
    "\n",
    "You should use Grid Search if the space is small and want to make sure you completely cover it to get the best hyperparameter combination within the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Hyperparameter Search Mind Map</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/hyperparamter optimization.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Manual Search\n",
    "\n",
    "(sidebar - fidelity is quality of information. Multi-fidelity serach changes the quality of information during the search process. Start with crude information to quickly understand the general search space, then add higher quality information to make better decisions later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- If possible, use 2nd order optimizations.\n",
    "- Hyperparameter is the outer loop optimization, thus can be a very slow.\n",
    "- Grid search is enumerating all possible hyperparameter options and testing each.\n",
    "- Random search is fixed number of random values for hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "\n",
    "https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/\n",
    "\n",
    "https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_random_search/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Halving search to speed up search</h2></center>\n",
    "\n",
    "[HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html) and [HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV) can be used as drop-in replacement for GridSearchCV and RandomizedSearchCV.\n",
    "\n",
    "> The search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_24_0.html#successive-halving-estimators-for-tuning-hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> Order Optimization Review</h2></center>\n",
    "\n",
    "<center><img src=\"../images/1_WKE8FNkYjqJk_qfeuvio6A.png\" width=\"45%\"/></center>\n",
    "\n",
    "- Most common\n",
    "- Efficient on per-iteration basis.\n",
    "- However, overall slow.\n",
    "- No strong guarantees on finding optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Image Source: https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculus Refresher</h2></center>\n",
    "\n",
    "<center><img src=\"../images/First-order-and-Second-order-derivatives-of-a-function-applied-to-an-edge-shown-by-the.png\" width=\"55%\"/></center>\n",
    "\n",
    "- The function itself.\n",
    "- 1st order derivative - The rate of change of the function.\n",
    "- 2nd order derivative -  The rate of change of the slope of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Loss Function</h2></center>\n",
    "\n",
    "<center><img src=\"../images/loss.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1st Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"../images/first.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2nd Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"../images/second.png\" width=\"100%\"/></center>\n",
    "\n",
    "Thus, 2<sup>nd</sup> order optimization has stronger guarantees on finding optimal solution because it can avoid local minimum in non-convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"../images/vs.png\" width=\"75%\"/></center>\n",
    "\n",
    "2<sup>nd</sup> order optimization is faster to converge because it takes steps based on the rate of curve change.\n",
    "\n",
    "In other words, it is a full step jumps directly to the minimum of the local squared approx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Gradient Descent is a 1<sup>st</sup> order technique that must take steps along the gradient.\n",
    "\n",
    "Netwon's method is a 2<sup>nd</sup> order technique that can take steps along the gradient of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/hessian.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Hessian is the matrix of second partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2nd order Optimizers Limitations</h2></center>\n",
    "\n",
    "2nd order optimizers are not as common as 1st order optimizations because of the high cost of computing the second-order information.\n",
    " \n",
    "Inverting the Hessian is $O(n^3)$. Most 2nd order methods often use approximations which are much faster $O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_4_Newtons.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Learn more about Logistic Regression solvers for scikit-learn](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sources:\n",
    "    \n",
    "- https://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/13-Optimization/04-secondOrderOpt.pdf\n",
    "- http://www.jmlr.org/papers/volume18/16-491/16-491.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>lbfgs Solver for Logistic Regression</h2></center>\n",
    "\n",
    "<center><img src=\"../images/lbfgs.png\" width=\"55%\"/></center>\n",
    "\n",
    "- `lbfgs` means Limited-memory Broyden–Fletcher–Goldfarb–Shanno. \n",
    "- Approximates the second derivative matrix updates. \n",
    "- Stores only a few vectors that represent the approximation implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>lbfgs Solver for Logistic Regression</h2></center>\n",
    "\n",
    "- Tends to be the best (effective and efficient) for small datasets \n",
    "    - Small n so the loss function can be quickly evaluated.\n",
    "    - Small p because only a few vectors are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Gradient-based optimization for hyperparameters</h2></center>\n",
    "\n",
    "Started in Deep Learning. Has been extended to SVM And Logistic Regression\n",
    "\n",
    "<center><h2>Hyperband</h2></center>\n",
    "\n",
    "Hyperband is a variation of random search, but with some explore-exploit theory to find the best time allocation for each of the configurations. You can check [this research paper](https://arxiv.org/abs/1603.06560) for further references.\n",
    "\n",
    "<center><h2>Bayesian Optimization</h2></center>\n",
    "\n",
    "Build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function.\n",
    "\n",
    "Good place to start learning Bayesian Optimization:\n",
    "\n",
    "+ [video](https://www.youtube.com/watch?v=J6UcAdH54RE)\n",
    "+ [slides](https://www.slideshare.net/SigOpt/using-bayesian-optimization-to-tune-machine-learning-models-62205716)\n",
    "\n",
    "<center><h2>Tree-structured Parzen estimators (TPE)</h2></center>\n",
    "\n",
    "The idea of Tree-based Parzen optimization is similar to Bayesian optimization. Instead of finding the values of p(y|x) where y is the function to be minimized (e.g., validation loss) and x is the value of hyperparameter the TPE models P(x|y) and P(y). One of the great drawbacks of tree-structured Parzen estimators is that they do not model interactions between the hyper-parameters. That said TPE works extremely well in practice and was battle-tested across most domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-optimize.github.io/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
