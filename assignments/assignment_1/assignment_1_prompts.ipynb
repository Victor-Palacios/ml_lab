{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "practical-ballot",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-1\">Instructions</a></span></li><li><span><a href=\"#Rubric\" data-toc-modified-id=\"Rubric-2\">Rubric</a></span></li><li><span><a href=\"#Hints\" data-toc-modified-id=\"Hints-3\">Hints</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-score",
   "metadata": {},
   "source": [
    "Instructions\n",
    "--------\n",
    "\n",
    "- This is a team assignment. Teams are randomly assigned through Canvas. A single notebook will be submitted per team.\n",
    "- The only resources you can use are:\n",
    "    * Course materials\n",
    "    * scikit-learn documentation\n",
    "    * Your team mates\n",
    "- You should not use or reference anything else, including, but not limited to, non-team classmates and Internet websites.\n",
    "- Complete each TODO.\n",
    "- No other imports are allowed in this notebook.\n",
    "- Do all cross validation and hyperparmater tunning by hand (we'll automate those in future assignments).\n",
    "- Prediction on unseen is the primary goal of machine learning, thus a significant number of points are based on your model's performance on hold-out data that you do not have access to.\n",
    "- See [coding guidelines](https://github.com/brianspiering/ml_lab/blob/main/resources/coding_guidelines.md), [coding review rubric](https://github.com/brianspiering/ml_lab/blob/main/resources/code_review_rubric.md), and course materials for how code will be evaluated.\n",
    "\n",
    "Rubric\n",
    "-------\n",
    "\n",
    "Total: __ / 14 points\n",
    "\n",
    "- __ / 2 points - Followed all directions as written and intended.\n",
    "- __ / 1 point - Code runs completely on first try on instructor's computer.\n",
    "- __ / 1 point - Evidence of trying different linear models.\n",
    "- __ / 1 point - Evidence of hand tuning hyperparameters.\n",
    "- __ / 1 point - Meets all coding guidelines / Idiomatic scikit-learn code.\n",
    "- __ / 2 points - Passed Level 1 test set performance. These point are all or none passed on tests.\n",
    "- __ / 2 points - Passed Level 2 test set performance. These point are all or none passed on tests.\n",
    "- __ / 2 points - Passed Level 3 test set performance. These point are all or none passed on tests.\n",
    "\n",
    "\n",
    "Hints\n",
    "------\n",
    "\n",
    "- Each person should attempt it individually. Then combine individual efforts in Deepnote.\n",
    "- Do not set random seed. You want your model to be robust to variance.\n",
    "- Sometimes there are bugs in packages. It is not your job to patch the bugs.\n",
    "- Perform EDA in a separate notebook. I suggest the `pandas_profiling` package\n",
    "    ```\n",
    "    from pandas_profiling import ProfileReport\n",
    "    profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "    profile.to_widgets()\n",
    "    profile.to_file(\"Pandas Profiling Report.html\")\n",
    "    ```\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "color-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tracked-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Do NOT import anything else\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import *\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.preprocessing   import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cubic-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "august-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train multiple linear model and show evidence of hand tuning hyperparameters.\n",
    "# This code should be commeted out. It will not be run to save time. \n",
    "# The code style and logic will be graded by visual inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "annoying-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dictionary of hand-tuned hyperparameters of final model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "imperial-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a single final model with dictionary of hand-tuned hyperparameters. This single final model will be then evaluated against test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "computational-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect pipe object\n",
    "assert \"pipe\" in dir()\n",
    "assert type(pipe) == Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accredited-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Level 1 test set performance\n",
      "Passed Level 2 test set performance\n",
      "Passed Level 3 test set performance\n"
     ]
    }
   ],
   "source": [
    "# Evaluate final model on test set\n",
    "# This code is commented out because you do have access to the test set\n",
    "\n",
    "# X_test = pd.read_csv(\"instructor/assignment_1_X_test.csv\", header=0)\n",
    "# y_test = pd.read_csv(\"instructor/assignment_1_y_test.csv\", header=0)\n",
    "# y_test = y_test.values.ravel()\n",
    "# y_pred = pipe.predict(X_test)\n",
    "# mse_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# assert mse_test <= 0.018870\n",
    "# print(\"Passed Level 1 test set performance ðŸ™‚\")\n",
    "\n",
    "# assert mse_test <= 0.017700\n",
    "# print(\"Passed Level 2 test set performance ðŸ™‚\")\n",
    "\n",
    "# assert mse_test <= 0.016877\n",
    "# print(\"Passed Level 3 test set performance ðŸ™‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-agency",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
